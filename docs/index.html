<html>
<head>
    <title>RCOS Microfossil Sorter</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <link href="static/style.css" rel="stylesheet">
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
</head>

<body>
<div class="jumbotron">
    <div class="backdrop">
    </div>
    <h1 class="display-3">Microfossil Concentrate Sorter</h1>
    <p class="lead">Olivia Lundelius and John Klingelhofer - RCOS Spring 2018</p>
    <a href="https://github.com/RCOS-MCFS/RCOS-Microfossil-Sorter" class="btn">View on GitHub</a>
</div>

<div class="stretch">
    <div class="container">
<div class="overview">
    <h2>Overview</h2>
    In paleontology, microfossil concentrate is a mixture of microfossils (typically from rodents and lizards) and gravel, with each particle being about 2 mm in size. Microfossils can potentially serve as a source of big data for paleontologists about the locations and environments from which they were collected. In order to study the microfossils, they must be sorted out from the gravel, which is a very time consuming process. This project aims to build a machine that automatically sorts microfossils from concentrate, and for that machine's assembly and software to be made open source and accessible by paleontology laboratories.
</div>
    </div>
</div>


<div class="container">
    <div class="blog-entry">
        <h2 class="blog-title"> Week 6, 7, 8, 9 </h2>
        <h4 class="blog-date">April 3rd, 2018</h4>
        <p class="blog-body">
            <h4> Long time, no see.</h4>
            Phew! Long time since our last blog post, which was done before spring break. But, this silence was not an
            indicator of a lack of activity, but rather a symbol of our focus on our work! These few weeks have been
            spent with leaning much more heavily into the visual component for the purpose of our demo which
            was demonstrated in our class presentation last week, and working on a version of our project better suited
            for laboratory settings.
            <br><br>
            <h4> What we've done. </h4>
            Before we get to the contents of our demo, it's worth touching on the advancements in our technology that
            have been made. As I started to lean into the live webcam based utility, it rapidly became apparent that
            the algorithm I'd made for a quick and dirty image cropping was entirely too slow for use with a webcam,
            as it slowed the display of the webcam's feed to about one frame a second. Much to my delight, and
            partial annoyance that I hadn't checked for this option before leaning so heavily into my own approach,
            it's relatively easy to string together a few OpenCV functions to detect contours in images, as well as
            some built in functions for checking the areas of these coordinates, and other such things. And most
            importantly, this function blew mine away in terms of speed.
            <br><br>
            With functions for these contours added to the img_tools library, they were applied in two settings:
            gathering data, and live labeling of data. The data gathering program is relatively simple, you
            run it, it shows a feed of webcam, and highlights the largest contour in the image. If the lighting environment
            is set up correctly, this should be the only, or largest, bone/rock in the frame. Then, the user presses
            'r' for 'b' to save the contents of that highlighted contour to a folder for rocks and bones respectively.
            <br><br>
            With the data gathered, we run the live data labeler. Running this file, currently live_demo.py though
            it will be soon be refactored into a more elegant form, the user specifies the type of model they wish to
            use, as well as the folders for the bone and rock data being used to train it. The specified model will then
            convert these images into the appropriate format, and train on a portion of this data and print accuracy, to
            give the user a baseline of how accurate it will be on live data. Then, it shows the webcam feed, labeling
            rocks and bones as needed.
            <br><br>
            <h4> What's next? </h4>
            Aside from the hardware, which Olivia will probably post an update on in the next week or so, upcoming software
            tasks will be:<ul>
                <li>
                    Escape the command line! Since we want this to be as usable as possible, we'll work on making
                    a more traditional UI for selecting all of what are currently command line options.
                </li>
                <li>
                    Refactor our demo, which is currently a  long script, into a series of methods, for easier use and
                    call by the aforementioned new UI, and easier extensibility in the future.
                </li>
                <li>
                    Set up some utility for changing the image thresholding depending on lighting
                    conditions, as our in-class demo classifier was ineffective because the lighting differed so much
                    from the environment in which the currently hard-coded parameters for the threshold were established.
                    Unfortunately, the typical approach of using a single object as a baseline with a saved perfect threshold
                    for that object isn't entirely feasible in our case, since the microfossils are so incredibly small
                    I can't fathom what common object could be used as this baseline, especially since no 3d printer could print
                    something to that side. We could go with an "eye-doctor" type approach for establishing the threshold, where
                    it cycles through various parameters asking the user if it looks better or worse before settling on a final option.
                    Food for thought!
                </li>
                <li>
                    Model improvement. I, because it was fun more than it was practical, wrote some of the models used
                    from their equations, and should probably transfer these methods over to applications of these models
                    which exist in certain ML libraries which are likely to be faster.
                </li>
                <li>
                    Additional data gathering. With binary classification pretty much done, we should start examining
                    additional details that can be automatically gathered.
                </li>
            </ul>

        </p>
        <p class="signature"> - John</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 5 </h2>
        <h4 class="blog-date">March 6, 2018</h4>
        <p class="blog-body">
            Recently got a micro servo motor (SG51R).  Seems fairly slow, but that shouldn't be too much of a problem, hopefully.
        </p><p>
            I've decided to greatly simplify the design for the sorter to more closely resemble the one created by Tom Kaye.  In this case,
            we'll need only one servo (hopefully the one we have should suffice) and will only need minimal assembly and designing.  This
            way, all we'll have to make is a hopper, a path for a sample to go on, and determine the distance needed for the sample to go
            in order to give the algorithm and servo enough time to determine where to put the sample.
        </p><p>
            UV LEDs arrived, we'll be able to assemble a lot more now.  Hardware-wise, the only things left are the hopper, path, and some
            kind of material for photographing the samples against.  This break, I (Olivia) will be able to set up my R-Pi with the servo
            HAT and a dummy LED, and hopefully put together a script that incorporates both the discrimination algorithm and controlling the
            servo.  John has a small web cam that works with the R-Pi, which should be useful.  So far as designing hardware, I'll try to
            make something over break but I'm not sure how successful I'll be.  Probably I'll just go to the Forge again and see if they can
            help, don't want to have to print a whole bunch of garbage before finally getting it right.
        </p>
        <p class="signature"> - Olivia</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 4 </h2>
        <h4 class="blog-date">February 27, 2018</h4>
        <p class="blog-body">
            Went to RPI Forge, which was extremely helpful.  Forge is mostly for 3D printing, the Embedded Hardware Club would 
            likely be more helpful for what we're doing, but someone at Forge helped me.  Apparently an Arduino isn't actually all
            that necessary, and the same things can be controlled by various modules that can connect to a Raspberry Pi directly rather
            than having the extra complication of going through an Arduino and trying to sync up a camera module through R-Pi and motor
            modules through Arduino.
        </p><p>
            It was suggested that we 3D print a hopper through Forge, I've been offered help with that by an individual named Misha.
            We should probably have two kinds of servos, a continuous one for the central rotating mechanism, and a positional rotation servo
            for a chute to deposit samples into one bin or another.
        </p><p>
            So far as getting everything to be used directly through the R-Pi, there's a small hardware piece that can be used directly
            through the R-Pi rather than be forced to use the Arduino interface.  There's a Python package (something like python-smbus)
            that works for this servo HAT after enabling I^2C on the R-Pi.  Not sure how to do any of that yet, will be working this week
            to get that working, and also to get two servos and the servo HAT.  It was suggested that we laser-cut the device out of
            polystyrene, but that's something further off into the future.  It was also suggested that we get a small spectrometer module,
            which, according to Misha, works very well.  Should try using that either instead of or in addition to the camera module.
        </p>
        <p class="signature"> - Olivia</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 4 </h2>
        <h4 class="blog-date">February 26, 2018</h4>
        <p class="blog-body">
            SVM classifier was fixed, now looking further into gathering hardware components. Ordered an Elegoo board on Amazon,
            as well as some wires, a breadboard and other components to start off.  This week we'll look at the performance
            of the classification algorithms on the Raspberry Pi (which should be just fine) and start using the Elegoo (which is
            exactly the same as an Arduino, just under a different name).
        </p><p>
            Am going to go to RPI Foundry to see what kind of help they can give us on our project, any hardware suggestions and
            maybe even find a Mech-E who's willing to join and help us.
        </p>
        <p class="signature"> - Olivia</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 3 </h2>
        <h4 class="blog-date">February 15, 2018</h4>
        <p class="blog-body">
            This week's is among the final weeks for the classifier, unless later in the semester we move beyond binary
            classification. There's now a function in place which can take an image containing multiple bones/rocks
            or a single bone/rock and crop an image containing just that image, provided that the image is taken on a
            clear enough background such that edge detection can form a general outline of the rock/stone in question.
        </p><p>
            This cropping may possibly prove useful as an additional metric in classification, as the rocks tend to have
            a much more uniform aspect ratio (often about 1:1) when compared to the bones. In addition, this automatic
            cropping makes obtaining measurements for the samples easy, which itself could prove an interesting
            additional metric. Though the clear difference in color between the rocks and bones under UV light may
            make these metrics superfluous, they may still have some purpose or have a role in broader analysis of
            collected samples.
        </p><p>
            This week also saw more work on different classifiers. Since differences in average color should be enough
            to segment the data, we developed a simple perceptron, and began work on a SVM. We also changed the structure
            of functions within files, with all of the image manipulations being moved into the "img_tools.py" file which
            can be imported into any function, rather than each classifier having its own subset of a broader collection
            of functions.
        </p>
        <p class="signature"> - John</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 2 </h2>
        <h4 class="blog-date">February 8, 2018</h4>
        <p class="blog-body">
            This week's work has largely been focused on the Computer Vision aspect of this project. Of course an important part of this process was the creation of new training data under more controlled conditions, as well as examining the various modes available for the classification of this data.
        </p><p>
            Increasing the expected ease of this process, one of the major delimitating factors between bones and stone is the average color. Provided that this classiciation difference holds true for all of our upcoming samples, aside from those which we have currently, each image can be reduced to a simple vector containing the average R, G, and B values. With the normalcy of this data (not the case were we interpreting the images directly), we can apply standard data mining techniques to this data such as SVMs, Linear Regression, etc. Our old training data could be inearly separated based on these RGB values, and we expect this to hold true for the upcoming data.
        </p><p>
            This upcoming week will include nailing down our classification strategy for future weeks, as well as starting to make some headway and decisions for the hardware aspect of this project.
        </p>
        <p class="signature"> - John</p>
    </div>
    <hr />
    <div class="blog-entry">
        <h2 class="blog-title"> Week 1 </h2>
        <h4 class="blog-date">February 1, 2018</h4>
        <p class="blog-body">
            This first week was largely focused on laying the foundation for this project, as a strong early foundation means time saved later on. This foundation week included the setup of our GitHub and this website, installing all of the packages to be used in this assignment on the comptuers of both team members (including the often persnickity OpenCV) and discussing some of the broader goals for the project.
        </p>
        <p class="signature"> - John</p>
    </div>
</div>
</body>
</html>
